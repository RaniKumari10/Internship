{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbbc448",
   "metadata": {},
   "source": [
    "# Completed _ WEB SCRAPING – ASSIGNMENT 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e510b",
   "metadata": {},
   "source": [
    "### • Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of your choice.  \n",
    "• **You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get \n",
    "information about selenium Exceptions. You may visit following links:**  \n",
    "1. https://selenium-python.readthedocs.io/api.html \n",
    "2. https://www.guru99.com/exception-handling-selenium.html \n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-element- \n",
    "exception/38023345  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40d85b",
   "metadata": {},
   "source": [
    "### 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = \n",
    "https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos **You need to find following details:**  \n",
    "**A) Rank  \n",
    "B) Name  \n",
    "C) Artist  \n",
    "D) Upload date  \n",
    "E) Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61aa3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '17.', '18.', '19.', '20.', '21.', '22.', '23.', '24.', '25.', '26.', '27.', '28.', '29.', '30.'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 28.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['\"Baby Shark Dance\"[6]', '\"Despacito\"[9]', '\"Johny Johny Yes Papa\"[16]', '\"Bath Song\"[17]', '\"Shape of You\"[18]', '\"See You Again\"[21]', '\"Wheels on the Bus\"[26]', '\"Phonics Song with Two Words\"[27]', '\"Uptown Funk\"[28]', '\"Learning Colors – Colorful Eggs on a Farm\"[29]', '\"Gangnam Style\"[30]', '\"Masha and the Bear – Recipe for Disaster\"[35]', '\"Dame Tu Cosita\"[36]', '\"Axel F\"[37]', '\"Sugar\"[38]', '\"Roar\"[39]', '\"Counting Stars\"[40]', '\"Baa Baa Black Sheep\"[41]', '\"Sorry\"[42]', '\"Waka Waka (This Time for Africa)\"[43]', '\"Thinking Out Loud\"[44]', '\"Lakdi Ki Kathi\"[45]', '\"Dark Horse\"[46]', '\"Perfect\"[47]', '\"Faded\"[48]', '\"Let Her Go\"[49]', '\"Humpty the train on a fruits ride\"[50]', '\"Girls Like You\"[51]', '\"Bailando\"[52]', '\"Lean On\"[53]'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 28.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 [\"Pinkfong Baby Shark - children's songs\", 'Luis Fonsi', 'LooLoo Kids - nursery rhymes', 'Cocomelon - nursery rhymes', 'Ed Sheeran', 'Wiz Khalifa', 'Cocomelon - nursery rhymes', \"ChuChu TV - children's songs\", 'Mark Ronson', \"Miroshka TV - children's songs\", 'Psy', \"Get Movies - children's songs\", 'El Chombo', 'Crazy Frog', 'Maroon 5', 'Katy Perry', 'OneRepublic', 'Cocomelon - nursery rhymes', 'Justin Bieber', 'Shakira', 'Ed Sheeran', 'Jingle Toons', 'Katy Perry', 'Ed Sheeran', 'Alan Walker', 'Passenger', \"Kiddiestv Hindi - children's songs\", 'Maroon 5', 'Enrique Iglesias', 'Major Lazer'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'May 2, 2018', 'January 30, 2017', 'April 6, 2015', 'May 24, 2018', 'March 6, 2014', 'November 19, 2014', 'February 27, 2018', 'July 15, 2012', 'January 31, 2012', 'April 5, 2018', 'June 16, 2009', 'January 14, 2015', 'September 5, 2013', 'May 31, 2013', 'June 25, 2018', 'October 22, 2015', 'June 4, 2010', 'October 7, 2014', 'June 14, 2018', 'February 20, 2014', 'November 9, 2017', 'December 3, 2015', 'July 25, 2012', 'January 26, 2018', 'May 31, 2018', 'April 11, 2014', 'March 22, 2015'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['13.18', '8.23', '6.76', '6.33', '6.05', '5.98', '5.46', '5.42', '5.00', '4.94', '4.86', '4.55', '4.41', '4.00', '3.91', '3.84', '3.84', '3.73', '3.69', '3.68', '3.63', '3.63', '3.56', '3.51', '3.49', '3.48', '3.51', '3.45', '3.43', '3.43'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - children's songs</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>13.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - nursery rhymes</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV - children's songs</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV - children's songs</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies - children's songs</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Sorry\"[42]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[43]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Thinking Out Loud\"[44]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[45]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Perfect\"[47]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Faded\"[48]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[49]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[50]</td>\n",
       "      <td>Kiddiestv Hindi - children's songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Bailando\"[52]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lean On\"[53]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                                       \"Roar\"[39]   \n",
       "16  17.                             \"Counting Stars\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.                                      \"Sorry\"[42]   \n",
       "19  20.           \"Waka Waka (This Time for Africa)\"[43]   \n",
       "20  21.                          \"Thinking Out Loud\"[44]   \n",
       "21  22.                             \"Lakdi Ki Kathi\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.                                    \"Perfect\"[47]   \n",
       "24  25.                                      \"Faded\"[48]   \n",
       "25  26.                                 \"Let Her Go\"[49]   \n",
       "26  27.          \"Humpty the train on a fruits ride\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                   \"Bailando\"[52]   \n",
       "29  30.                                    \"Lean On\"[53]   \n",
       "\n",
       "                                    Artist        Upload Date  Views  \n",
       "0   Pinkfong Baby Shark - children's songs      June 17, 2016  13.18  \n",
       "1                               Luis Fonsi   January 12, 2017   8.23  \n",
       "2             LooLoo Kids - nursery rhymes    October 8, 2016   6.76  \n",
       "3               Cocomelon - nursery rhymes        May 2, 2018   6.33  \n",
       "4                               Ed Sheeran   January 30, 2017   6.05  \n",
       "5                              Wiz Khalifa      April 6, 2015   5.98  \n",
       "6               Cocomelon - nursery rhymes       May 24, 2018   5.46  \n",
       "7             ChuChu TV - children's songs      March 6, 2014   5.42  \n",
       "8                              Mark Ronson  November 19, 2014   5.00  \n",
       "9           Miroshka TV - children's songs  February 27, 2018   4.94  \n",
       "10                                     Psy      July 15, 2012   4.86  \n",
       "11           Get Movies - children's songs   January 31, 2012   4.55  \n",
       "12                               El Chombo      April 5, 2018   4.41  \n",
       "13                              Crazy Frog      June 16, 2009   4.00  \n",
       "14                                Maroon 5   January 14, 2015   3.91  \n",
       "15                              Katy Perry  September 5, 2013   3.84  \n",
       "16                             OneRepublic       May 31, 2013   3.84  \n",
       "17              Cocomelon - nursery rhymes      June 25, 2018   3.73  \n",
       "18                           Justin Bieber   October 22, 2015   3.69  \n",
       "19                                 Shakira       June 4, 2010   3.68  \n",
       "20                              Ed Sheeran    October 7, 2014   3.63  \n",
       "21                            Jingle Toons      June 14, 2018   3.63  \n",
       "22                              Katy Perry  February 20, 2014   3.56  \n",
       "23                              Ed Sheeran   November 9, 2017   3.51  \n",
       "24                             Alan Walker   December 3, 2015   3.49  \n",
       "25                               Passenger      July 25, 2012   3.48  \n",
       "26      Kiddiestv Hindi - children's songs   January 26, 2018   3.51  \n",
       "27                                Maroon 5       May 31, 2018   3.45  \n",
       "28                        Enrique Iglesias     April 11, 2014   3.43  \n",
       "29                             Major Lazer     March 22, 2015   3.43  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "# # Importing required Exceptions which needs to handled\n",
    "# from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def scrape_most_viewed_videos(url):\n",
    "    \n",
    "    #Creates Chrome WebDriver instance.\n",
    "    driver = webdriver.Chrome()\n",
    "    #Let's maximize the automated chrome window\n",
    "    driver.maximize_window()\n",
    "\n",
    "    #Navigate the Goole Map Website\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    search= driver.find_element(By.XPATH, '//*[@id=\"noarticletext\"]/tbody/tr/td/span/a')\n",
    "    search.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    search_1 = driver.find_element(By.XPATH, '//*[@id=\"mw-content-text\"]/div[3]/div[4]/ul/li[1]/table/tbody/tr/td[2]/div[1]/a')       # Locating page for top youtube videos by xpath\n",
    "    search_1.click()\n",
    "    \n",
    "    rank = []\n",
    "    name = []\n",
    "    artist = []\n",
    "    upload_date = []\n",
    "    views = []\n",
    "    \n",
    "    #scraping the Rank \n",
    "    rk=driver.find_elements(By.XPATH, \"//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr/td[1]\")\n",
    "    for i in tqdm(rk):\n",
    "        if i.text is None :\n",
    "            rank.append(\"--\") \n",
    "        else:\n",
    "            rank.append(i.text)\n",
    "    print(len(rank),rank,'\\n')\n",
    "    \n",
    "    #scraping the Video Name \n",
    "    nm=driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr/td[2]\")\n",
    "    for i in tqdm(nm):\n",
    "        if i.text is None :\n",
    "            name.append(\"--\") \n",
    "        else:\n",
    "            name.append(i.text)\n",
    "    print(len(name),name,'\\n')\n",
    "    \n",
    "    #scraping the Artist \n",
    "    Ar=driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr/td[3]\")\n",
    "    for i in tqdm(Ar):\n",
    "        if i.text is None :\n",
    "            artist.append(\"--\") \n",
    "        else:\n",
    "            artist.append(i.text)\n",
    "    print(len(artist),artist,'\\n')\n",
    "    \n",
    "    #scraping the Upload_Date \n",
    "    date=driver.find_elements(By.XPATH, \"//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr/td[5]\")\n",
    "    for i in tqdm(date):\n",
    "        if i.text is None :\n",
    "            upload_date.append(\"--\") \n",
    "        else:\n",
    "            upload_date.append(i.text)\n",
    "    print(len(upload_date),upload_date,'\\n')\n",
    "    \n",
    "    #scraping the Views \n",
    "    v=driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr/td[4]\")\n",
    "    for i in tqdm(v):\n",
    "        if i.text is None :\n",
    "            views.append(\"--\") \n",
    "        else:\n",
    "            views.append(i.text)\n",
    "    print(len(views),views,'\\n')\n",
    "       \n",
    "    df = pd.DataFrame({'Rank':rank,'Name':name, 'Artist':artist, 'Upload Date':upload_date, 'Views':views})\n",
    "    display(df)\n",
    "             \n",
    "scrape_most_viewed_videos(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3a630",
   "metadata": {},
   "source": [
    "### 2. Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.  \n",
    "**You need to find following details:**  \n",
    "\n",
    "**A) Match title (I.e. 1 ODI)  \n",
    "B) Series  \n",
    "C) Place  \n",
    "D) Date  \n",
    "E) Time  \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e68d551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 50.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['ASIA CUP 2023', 'ASIA CUP 2023', 'AUSTRALIA TOUR OF INDIA 2023-24', 'AUSTRALIA TOUR OF INDIA 2023-24', 'AUSTRALIA TOUR OF INDIA 2023-24', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 69.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['1st ODI -', '2nd ODI -', '1st ODI -', '2nd ODI -', '3rd ODI -', '1st ODI -', '2nd ODI -', '3rd ODI -']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 28.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['6:30 AM CEST', '6:30 AM CEST', '10:00 AM CEST', '10:00 AM CEST', '10:00 AM CEST', '10:30 AM CEST', '10:30 AM CEST', '10:30 AM CEST']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#def scrape_most_viewed_videos(url):\n",
    "    \n",
    "#Creates Chrome WebDriver instance.\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Let's maximize the automated chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "#Navigate the Goole Map Website\n",
    "time.sleep(1)\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "search= driver.find_element(By.XPATH, '//*[@id=\"navigation\"]/ul[1]/li[2]/a')\n",
    "search.click()\n",
    "#driver.execute_script(\"arguments[0].click()\",search)\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "cookies = driver.find_element(By.XPATH, '/html/body/div[7]/button')\n",
    "#driver.execute_script(\"arguments[0].click()\",cookies)\n",
    "cookies.click()\n",
    "search_1 = driver.find_element(By.XPATH, '//*[@id=\"fixtures\"]/div[2]/div/div[3]/div')       # Locating page for top youtube videos by xpath\n",
    "search_1.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "search_2 = driver.find_element(By.XPATH, '//*[@id=\"fixtures\"]/div[2]/div/div[3]/div/div[2]/div[3]')       # Locating page for top youtube videos by xpath\n",
    "search_2.click()\n",
    "       \n",
    "match_title = []\n",
    "series = []\n",
    "place = []\n",
    "date = []\n",
    "time = []\n",
    "\n",
    "#scraping the match_title \n",
    "mt = driver.find_elements(By.XPATH, '//*[@id=\"match-card\"]/div[1]/h5')\n",
    "for i in tqdm(mt):\n",
    "    if i.text is None :\n",
    "        match_title.append(\"--\") \n",
    "    else:\n",
    "        match_title.append(i.text)\n",
    "print(len(match_title),match_title)\n",
    "\n",
    "#scraping the series \n",
    "sri = driver.find_elements(By.XPATH, '//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "for i in tqdm(sri):\n",
    "    if i.text is None:\n",
    "        series.append(\"--\")\n",
    "    else:\n",
    "        series.append(i.text)\n",
    "print(len(series),series)\n",
    "\n",
    "#scraping the place\n",
    "pla = driver.find_elements(By.XPATH, '/html/body/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div/div/div[3]/div/span[2]')\n",
    "for i in tqdm(place):\n",
    "    if i.text is None:\n",
    "        place.append(\"--\")\n",
    "    else:\n",
    "        place.append(i.text)\n",
    "print(len(place),place)\n",
    "\n",
    "#scraping the date\n",
    "da = driver .find_elements(By.XPATH, '//*[@id=\"match-card\"]/div[1]/div/div[1]')\n",
    "for i in tqdm(date):\n",
    "    if i.text is None:\n",
    "        date.append(\"--\")\n",
    "    else:\n",
    "        date.append(i.text)\n",
    "print(len(date), date)\n",
    "\n",
    "#scraping the time\n",
    "ti = driver.find_elements(By.XPATH, '//*[@id=\"match-card\"]/div[1]/div/div[2]')\n",
    "for i in tqdm(ti):\n",
    "    if i.text is None:\n",
    "        time.append(\"--\")\n",
    "    else:\n",
    "        time.append(i.text)\n",
    "print(len(time), time)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba228ab",
   "metadata": {},
   "source": [
    "### 3. Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/  \n",
    "**You have to find following details:** \n",
    "**A) Rank  \n",
    "B) State  \n",
    "C) GSDP(18-19)- at current prices  \n",
    "D) GSDP(19-20)- at current prices  \n",
    "E) Share(18-19)  \n",
    "F) GDP($ billion)** \n",
    "**Note: - From statisticstimes home page you have to reach to economy page through code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053699cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:04<00:00,  6.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 47.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 50.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 54.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 40.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 54.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP_18_19</th>\n",
       "      <th>GSDP_19_20</th>\n",
       "      <th>Share_18_19</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP_18_19 GSDP_19_20 Share_18_19      GDP\n",
       "0     1                Maharashtra  2,632,792          -      13.94%  399.921\n",
       "1     2                 Tamil Nadu  1,630,208  1,845,853       8.63%  247.629\n",
       "2     3              Uttar Pradesh  1,584,764  1,687,818       8.39%  240.726\n",
       "3     4                    Gujarat  1,502,899          -       7.96%  228.290\n",
       "4     5                  Karnataka  1,493,127  1,631,977       7.91%  226.806\n",
       "5     6                West Bengal  1,089,898  1,253,832       5.77%  165.556\n",
       "6     7                  Rajasthan    942,586  1,020,989       4.99%  143.179\n",
       "7     8             Andhra Pradesh    862,957    972,782       4.57%  131.083\n",
       "8     9                  Telangana    861,031    969,604       4.56%  130.791\n",
       "9    10             Madhya Pradesh    809,592    906,672       4.29%  122.977\n",
       "10   11                     Kerala    781,653          -       4.14%  118.733\n",
       "11   12                      Delhi    774,870    856,112       4.10%  117.703\n",
       "12   13                    Haryana    734,163    831,610       3.89%  111.519\n",
       "13   14                      Bihar    530,363    611,804       2.81%   80.562\n",
       "14   15                     Punjab    526,376    574,760       2.79%   79.957\n",
       "15   16                     Odisha    487,805    521,275       2.58%   74.098\n",
       "16   17                      Assam    315,881          -       1.67%   47.982\n",
       "17   18               Chhattisgarh    304,063    329,180       1.61%   46.187\n",
       "18   19                  Jharkhand    297,204    328,598       1.57%   45.145\n",
       "19   20                Uttarakhand    245,895          -       1.30%   37.351\n",
       "20   21            Jammu & Kashmir    155,956          -       0.83%   23.690\n",
       "21   22           Himachal Pradesh    153,845    165,472       0.81%   23.369\n",
       "22   23                        Goa     73,170     80,449       0.39%   11.115\n",
       "23   24                    Tripura     49,845     55,984       0.26%    7.571\n",
       "24   25                 Chandigarh     42,114          -       0.22%    6.397\n",
       "25   26                 Puducherry     34,433     38,253       0.18%    5.230\n",
       "26   27                  Meghalaya     33,481     36,572       0.18%    5.086\n",
       "27   28                     Sikkim     28,723     32,496       0.15%    4.363\n",
       "28   29                    Manipur     27,870     31,790       0.15%    4.233\n",
       "29   30                   Nagaland     27,283          -       0.14%    4.144\n",
       "30   31          Arunachal Pradesh     24,603          -       0.13%    3.737\n",
       "31   32                    Mizoram     22,287     26,503       0.12%    3.385\n",
       "32   33  Andaman & Nicobar Islands          -          -           -        -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, ElementNotInteractableException \n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def scrape_state_gdp_details(url):\n",
    "\n",
    "    #Creates Chrome WebDriver instance.\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    #Let's maximize the automated chrome window\n",
    "    driver.maximize_window()\n",
    "\n",
    "    #Navigate the Goole Map Website\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    India=driver.find_element(By.XPATH,'//div[@class=\"dropdown\"][2]/div/a[3]')\n",
    "    try:\n",
    "        India.click()\n",
    "    except ElementNotInteractableException:\n",
    "        driver.get(India.get_attribute('href'))\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    state = driver.find_element(By.XPATH,'//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li[1]/a')\n",
    "    try:\n",
    "        state.click()\n",
    "    except ElementNotInteractableException:\n",
    "        driver.get(state.get_attribute('href'))\n",
    "    time.sleep(5)\n",
    "\n",
    "    cookies = driver.find_element(By.XPATH, '/html/body/div[2]/div/div/div/div[2]/div/button[2]')\n",
    "    cookies.click()\n",
    "\n",
    "    cookies_1 = driver.find_element(By.XPATH, '/html/body/div[1]/div/a')\n",
    "    driver.execute_script(\"arguments[0].click()\",cookies_1)\n",
    "\n",
    "    rank = []\n",
    "    state = []\n",
    "    gsdp_18_19 = []\n",
    "    gsdp_19_20 = []\n",
    "    share_18_19 = []\n",
    "    gdp = []\n",
    "\n",
    "    try: \n",
    "        rk = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "        for i in tqdm(rk):\n",
    "            rank.append(i.text)\n",
    "    except: \n",
    "        rank.append('Not Found')\n",
    "    \n",
    "    try: \n",
    "        st =driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "        for _ in tqdm(st):\n",
    "            state.append(_.text)\n",
    "    except: \n",
    "        state.append('__')\n",
    "    \n",
    "    try:\n",
    "        gsdp_1819 = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "        for j in tqdm(gsdp_1819):\n",
    "            gsdp_18_19.append(j.text)\n",
    "    except:\n",
    "        gsdp_18_19.append('--')\n",
    "    \n",
    "    try:\n",
    "        gsdp_1920 = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "        for k in tqdm(gsdp_1920):\n",
    "            gsdp_19_20.append(k.text)\n",
    "    except:\n",
    "        gsdp_19_20.append('NA')\n",
    "\n",
    "    try: \n",
    "        share_1819 = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "        for z in tqdm(share_1819):\n",
    "            share_18_19.append(z.text)\n",
    "    except:\n",
    "        share_18_19.append('__')\n",
    "    \n",
    "    try:\n",
    "        gdp_ = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "        for i in tqdm(gdp_):\n",
    "            gdp.append(i.text)\n",
    "    except:\n",
    "        gdp.append('__')\n",
    "    \n",
    "\n",
    "    print(len(rank), len(state), len(gsdp_18_19), len(gsdp_19_20), len(share_18_19), len(gdp),'\\n')\n",
    "\n",
    "    df = pd.DataFrame({'Rank':rank, 'State':state, 'GSDP_18_19':gsdp_18_19, 'GSDP_19_20':gsdp_19_20, 'Share_18_19':share_18_19, 'GDP':gdp})\n",
    "    display(df)\n",
    "        \n",
    "scrape_state_gdp_details(\"https://www.statisticstimes.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e1446",
   "metadata": {},
   "source": [
    "### 4. Scrape the details of trending repositories on Github.com. Url = https://github.com/  \n",
    "**You have to find the following details:  \n",
    "A) Repository title  \n",
    "B) Repository description  \n",
    "C) Contributors count  \n",
    "D) Language used**\n",
    "\n",
    "**Note: - From the home page you have to click on the trending option from Explore menu through code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a099df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 75.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['getumbrel', 'ill-inc', 'roboflow', 'elidianaandrade', 'a16z-infra', 'qiuyu96', 'StanGirard', 'byoungd', 'digininja', 'kubernetes-sigs', 'opentffoundation', 'ascoders', 'danswer-ai', 'chatchat-space', '1Panel-dev', 'google', 'openai', 'QiuChenlyOpenSource', 'kuafuai', 'yujiangshui', 'steven-tey', 'facebookresearch', 'jackfrued', 'google', 'hehonghui']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 94.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['A self-hosted, offline, ChatGPT-like chatbot. Powered by Llama 2. 100% private, with no data leaving your device.', 'Biomes is an open source sandbox MMORPG built for the web using web technologies such as Next.js, Typescript, React and WebAssembly.', 'We write your reusable computer vision tools. 💜', 'Repositório do lab Contribuindo em um Projeto Open Source no GitHub da Digital Innovation One.', 'A MIT-licensed, deployable starter kit for building and customizing your own version of AI town - a virtual town where AI characters live, chat and socialize.', 'Official PyTorch implementation of CoDeF: Content Deformation Fields for Temporally Consistent Video Processing', '🧠 Your Second Brain supercharged by Generative AI 🧠 Dump all your files and chat with your personal assistant on your files & more using GPT 3.5/4, Private, Anthropic, VertexAI, LLMs...', 'An advanced guide to learn English which might benefit you a lot 🎉 . 可能是让你受益匪浅的英语进阶指南。', 'Damn Vulnerable Web Application (DVWA)', 'Configure external DNS servers (AWS Route53, Google CloudDNS and others) for Kubernetes Ingresses and Services', \"The OpenTF Manifesto expresses concern over HashiCorp's switch of the Terraform license from open-source to the Business Source License (BSL) and calls for the tool's return to a truly open-source license.\", '前端精读周刊。帮你理解最前沿、实用的技术。', 'Ask Questions in natural language and get Answers backed by private sources. Connects to tools like Slack, GitHub, Confluence, etc.', 'Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain ｜ 基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答', '🔥 🔥 🔥 现代化、开源的 Linux 服务器运维管理面板。', 'GoogleTest - Google Testing and Mocking Framework', 'Robust Speech Recognition via Large-Scale Weak Supervision', '基于Ruby编写的命令行注入版本', 'Multi agent system for AI-driven software development. Convert natural language requirements into working software. Supports any development language and extends the existing base code.', '专为程序员编写的英语学习指南 v1.2。在线版本请点 ->', 'Notion-style WYSIWYG editor with AI-powered autocompletion.', 'Code to accompany \"A Method for Animating Children\\'s Drawings of the Human Figure\"', 'Python - 100天从新手到大师', 'Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more', '经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi、pdf格式, 每周更新']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 67.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by', 'Built by']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 70.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ['TypeScript', 'TypeScript', 'Python', 'TypeScript', 'Python', 'TypeScript', 'PHP', 'Go', 'HTML', 'JavaScript', 'Python', 'Python', 'Go', 'C++', 'Python', 'Shell', 'Python', 'TypeScript', 'Python', 'Python', 'Python', 'CSS']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def scrape_trending_repositories(url):\n",
    "    \n",
    "    chrome_options = Options()\n",
    "\n",
    "    chrome_options.add_argument('--incognito')\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "    #Let's maximize the automated chrome window\n",
    "\n",
    "    driver.maximize_window()\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    x = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "    driver.execute_script(\"arguments[0].click()\",x)\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    repository_title = []\n",
    "    repository_description = []\n",
    "    contributors_count = []\n",
    "    language_used = []  \n",
    "\n",
    "    #scraping the repository_title \n",
    "    try:\n",
    "        rt = driver.find_elements(By.XPATH, \"//span[@class='text-normal']\")\n",
    "        for i in tqdm(rt):\n",
    "            repository_title.append(i.text.split(' /')[0]) \n",
    "    except:\n",
    "        repository_title.append('__')\n",
    "    print(len(repository_title),repository_title)\n",
    "\n",
    "    #scraping the repository_description\n",
    "    try:\n",
    "        rd = driver.find_elements(By.XPATH, '/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/p')\n",
    "        for i in tqdm(rd):\n",
    "            repository_description.append(i.text)\n",
    "    except:\n",
    "        repository_description.append('NA')\n",
    "    print(len(repository_description),repository_description)\n",
    "\n",
    "    #scarping the contributors_count\n",
    "    try: \n",
    "        con =  driver.find_elements(By.XPATH, '//span[@class=\"d-inline-block mr-3\"]')\n",
    "        for i in tqdm(con):\n",
    "            contributors_count.append(i.text)\n",
    "    except:\n",
    "        contributors_count.append('__')\n",
    "    print(len(contributors_count), contributors_count)\n",
    "\n",
    "    #scraping the language_used\n",
    "    try:\n",
    "        lang = driver.find_elements(By.XPATH, \"//span[@itemprop='programmingLanguage']\")\n",
    "        for i in tqdm(lang):\n",
    "            language_used.append(i.text)\n",
    "    except:\n",
    "        language_used.append('NA')\n",
    "    print(len(language_used), language_used)\n",
    "    \n",
    "scrape_trending_repositories(\"https://github.com/\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35f6eb",
   "metadata": {},
   "source": [
    "### 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ \n",
    "**You have to find the following details:  \n",
    "A) Song name  \n",
    "B) Artist name  \n",
    "C) Last week rank  \n",
    "D) Peak rank  \n",
    "E) Weeks on board  \n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd9f69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Last Night', 'Fast Car', 'Cruel Summer', 'Calm Down', 'Fukumean', 'Vampire', 'Dance The Night', 'Barbie World', 'Flowers', 'Snooze', 'All My Life', 'Meltdown', 'Karma', 'What Was I Made For?', 'Paint The Town Red', 'Need A Favor', 'Religiously', 'Anti-Hero', 'Kill Bill', \"Creepin'\", 'Try That In A Small Town', 'I Know ?', \"Thinkin' Bout Me\", 'FE!N', 'Cupid', 'Favorite Song', 'K-POP', 'Seven', 'Telekinesis', 'Dial Drunk', 'Ella Baila Sola', 'Next Thing You Know', 'Love You Anyway', 'Thought You Should Know', 'Chemical', 'Bury Me In Georgia', 'You, Me, & Whiskey', 'La Bebe', 'Watermelon Moonshine', 'Search & Rescue', 'Put It On Da Floor Again', 'Un x100to', 'LaLa', 'My Eyes', 'Daylight', 'Topia Twins', 'Lady Gaga', \"Baby Don't Hurt Me\", 'Blank Space', 'What It Is (Block Boy)', 'Deli', 'Where She Goes', 'Princess Diana', 'Peaches & Eggplants', 'In Your Love', 'Mourning', 'Area Codes', 'Your Heart Or Mine', 'White Horse', 'Sabor Fresa', 'Truck Bed', 'Eyes Closed', \"I Can See You (Taylor's Version) (From The Vault)\", 'Thank God', 'Tulum', 'Super Shy', 'Popular', 'Good Good', 'Everything I Love', 'Hyaena', 'Modern Jam', 'Shake Sumn', 'Jaded', 'Stand By Me', 'Speed Drive', 'Fragil', 'TQM', \"Angels Don't Always Have Wings\", 'Johnny Dang', 'Memory Lane', 'Heartbroken', 'El Amor de Su Vida', 'Jealousy', 'Oh U Went', 'Til Further Notice', 'Oklahoma Smoke Show', 'Sirens', 'Save Me', \"God's Country\", \"I'm Just Ken\", 'ICU', 'Skitzo', 'Delresto (Echoes)', 'See You Again', 'Pound Town 2', 'Lagunas', 'Overdrive', 'Bzrp Music Sessions, Vol. 55', 'Dawns', 'Rubicon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Morgan Wallen', 'Luke Combs', 'Taylor Swift', 'Rema & Selena Gomez', 'Gunna', 'Olivia Rodrigo', 'Dua Lipa', 'Nicki Minaj & Ice Spice With Aqua', 'Miley Cyrus', 'SZA', 'Lil Durk Featuring J. Cole', 'Travis Scott Featuring Drake', 'Taylor Swift Featuring Ice Spice', 'Billie Eilish', 'Doja Cat', 'Jelly Roll', 'Bailey Zimmerman', 'Taylor Swift', 'SZA', 'Metro Boomin, The Weeknd & 21 Savage', 'Jason Aldean', 'Travis Scott', 'Morgan Wallen', 'Travis Scott Featuring Playboi Carti', 'Fifty Fifty', 'Toosii', 'Travis Scott, Bad Bunny & The Weeknd', 'Jung Kook Featuring Latto', 'Travis Scott Featuring SZA & Future', 'Noah Kahan With Post Malone', 'Eslabon Armado X Peso Pluma', 'Jordan Davis', 'Luke Combs', 'Morgan Wallen', 'Post Malone', 'Kane Brown', 'Justin Moore & Priscilla Block', 'Yng Lvcas x Peso Pluma', 'Lainey Wilson', 'Drake', 'Latto Featuring Cardi B', 'Grupo Frontera X Bad Bunny', 'Myke Towers', 'Travis Scott', 'David Kushner', 'Travis Scott Featuring Rob49 & 21 Savage', 'Peso Pluma, Gabito Ballesteros & Junior H', 'David Guetta, Anne-Marie & Coi Leray', 'Taylor Swift', 'Doechii Featuring Kodak Black', 'Ice Spice', 'Bad Bunny', 'Ice Spice & Nicki Minaj', 'Young Nudy Featuring 21 Savage', 'Tyler Childers', 'Post Malone', 'Kaliii', 'Jon Pardi', 'Chris Stapleton', 'Fuerza Regida', 'HARDY', 'Ed Sheeran', 'Taylor Swift', 'Travis Scott', 'Peso Pluma & Grupo Frontera', 'NewJeans', 'The Weeknd, Playboi Carti & Madonna', 'Usher, Summer Walker & 21 Savage', 'Morgan Wallen', 'Travis Scott', 'Travis Scott Featuring Teezo Touchdown', 'DaBaby', 'Miley Cyrus', 'Lil Durk Featuring Morgan Wallen', 'Charli XCX', 'Yahritza y Su Esencia x Grupo Frontera', 'Fuerza Regida', 'Thomas Rhett', 'That Mexican OT, Paul Wall & DRODi', 'Old Dominion', 'Diplo, Jessie Murph & Polo G', 'Grupo Frontera & Grupo Firme', 'Offset & Cardi B', 'Young Thug Featuring Drake', 'Travis Scott Featuring James Blake & 21 Savage', 'Zach Bryan', 'Travis Scott', 'Jelly Roll With Lainey Wilson', 'Travis Scott', 'Ryan Gosling', 'Coco Jones', 'Travis Scott Featuring Young Thug', 'Travis Scott & Beyonce', 'Tyler, The Creator Featuring Kali Uchis', 'Sexyy Red & Tay Keith & Nicki Minaj', 'Peso Pluma & Jasiel Nunez', 'Post Malone', 'Bizarrap & Peso Pluma', 'Zach Bryan Featuring Maggie Rogers', 'Peso Pluma']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1', '2', '4', '6', '7', '9', '10', '8', '12', '15', '13', '3', '20', '22', '-', '24', '33', '31', '29', '32', '21', '11', '37', '5', '41', '39', '18', '30', '26', '44', '42', '45', '48', '50', '35', '51', '47', '52', '71', '54', '57', '59', '60', '19', '74', '17', '63', '64', '-', '66', '58', '61', '62', '69', '43', '40', '65', '72', '79', '70', '91', '73', '76', '16', '75', '77', '86', '-', '83', '14', '23', '84', '93', '81', '78', '82', '85', '-', '100', '95', '94', '-', '55', '96', '38', '-', '27', '-', '28', '92', '97', '34', '25', '-', '98', '-', '68', '99', '-', '-']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1', '2', '3', '3', '4', '1', '7', '7', '1', '10', '2', '3', '2', '14', '15', '14', '17', '1', '1', '3', '1', '11', '9', '5', '17', '5', '7', '1', '26', '25', '4', '23', '15', '7', '13', '36', '37', '11', '39', '2', '13', '5', '43', '19', '45', '17', '35', '48', '1', '50', '41', '8', '4', '52', '43', '36', '33', '53', '31', '26', '61', '19', '5', '16', '43', '48', '43', '68', '14', '14', '23', '65', '56', '22', '73', '69', '34', '78', '79', '27', '64', '82', '55', '19', '38', '86', '27', '85', '28', '87', '62', '34', '25', '44', '66', '77', '47', '31', '42', '63']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['28', '20', '14', '49', '8', '6', '11', '7', '30', '35', '13', '2', '22', '4', '1', '19', '14', '42', '35', '36', '4', '2', '23', '2', '21', '25', '3', '4', '2', '8', '21', '29', '26', '52', '17', '13', '14', '21', '7', '18', '10', '17', '5', '2', '17', '2', '7', '12', '37', '14', '3', '12', '17', '10', '2', '12', '14', '13', '3', '7', '8', '20', '5', '2', '6', '5', '10', '1', '24', '2', '2', '12', '12', '11', '3', '16', '12', '3', '4', '19', '3', '1', '2', '7', '2', '3', '2', '8', '2', '3', '19', '2', '2', '16', '10', '6', '3', '10', '15', '6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Singer</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fukumean</td>\n",
       "      <td>Gunna</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lagunas</td>\n",
       "      <td>Peso Pluma &amp; Jasiel Nunez</td>\n",
       "      <td>-</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Overdrive</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bzrp Music Sessions, Vol. 55</td>\n",
       "      <td>Bizarrap &amp; Peso Pluma</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dawns</td>\n",
       "      <td>Zach Bryan Featuring Maggie Rogers</td>\n",
       "      <td>-</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rubicon</td>\n",
       "      <td>Peso Pluma</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Song Name                              Singer  \\\n",
       "0                     Last Night                       Morgan Wallen   \n",
       "1                       Fast Car                          Luke Combs   \n",
       "2                   Cruel Summer                        Taylor Swift   \n",
       "3                      Calm Down                 Rema & Selena Gomez   \n",
       "4                       Fukumean                               Gunna   \n",
       "..                           ...                                 ...   \n",
       "95                       Lagunas           Peso Pluma & Jasiel Nunez   \n",
       "96                     Overdrive                         Post Malone   \n",
       "97  Bzrp Music Sessions, Vol. 55               Bizarrap & Peso Pluma   \n",
       "98                         Dawns  Zach Bryan Featuring Maggie Rogers   \n",
       "99                       Rubicon                          Peso Pluma   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on Board  \n",
       "0               1         1             28  \n",
       "1               2         2             20  \n",
       "2               4         3             14  \n",
       "3               6         3             49  \n",
       "4               7         4              8  \n",
       "..            ...       ...            ...  \n",
       "95              -        77              6  \n",
       "96             68        47              3  \n",
       "97             99        31             10  \n",
       "98              -        42             15  \n",
       "99              -        63              6  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def scrape_billboard_top_songs(url):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "    #Let's maximize the automated chrome window\n",
    "\n",
    "    driver.maximize_window()\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    song_name =[]\n",
    "    singer=[]\n",
    "    last_week=[]\n",
    "    peak_pos = []\n",
    "    weeks_on_board=[]\n",
    "\n",
    "    x = driver.find_element(By.XPATH, '//button[@id=\"onetrust-reject-all-handler\"]')\n",
    "    x.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    top100 = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')       # Locating page foe top videos by xpath\n",
    "    top100.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    y = driver.find_element(By.XPATH, '//*[@id=\"main-wrapper\"]/main/div[2]/div[1]/div[1]/div/div/div[3]/a')\n",
    "    #y.click() : givving \n",
    "    driver.execute_script(\"arguments[0].click()\",y)\n",
    "\n",
    "    #scraping the song_name \n",
    "    son=driver.find_elements(By.XPATH, '/html/body/div[4]/main/div[2]/div[3]/div/div/div/div[2]/div/ul/li[4]/ul/li[1]/h3')#'need to chjeck the data'\n",
    "    for i in tqdm(son):\n",
    "        if i.text is None :\n",
    "            song_name.append(\"--\") \n",
    "        else:\n",
    "             song_name.append(i.text)\n",
    "    print(len(song_name),song_name)\n",
    "\n",
    "    #scraping the Singer \n",
    "    sin=driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div/ul/li[4]/ul/li[1]/span')\n",
    "    for i in tqdm(sin):\n",
    "        if i.text is None :\n",
    "            singer.append(\"--\") \n",
    "        else:\n",
    "            singer.append(i.text)\n",
    "    print(len(singer),singer)\n",
    "    \n",
    "    lwr=driver.find_elements(By.XPATH, '//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div/ul/li[4]/ul/li[4]/span')\n",
    "    for i in tqdm(lwr):\n",
    "        if i.text is None :\n",
    "            last_week.append(\"--\") \n",
    "        else:\n",
    "            last_week.append(i.text)\n",
    "    print(len(last_week),last_week)\n",
    "\n",
    "    #scraping the Peak_pos Rank \n",
    "    lwr1=driver.find_elements(By.XPATH, '//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div/ul/li[4]/ul/li[5]/span')\n",
    "    for i in tqdm(lwr1):\n",
    "        if i.text is None :\n",
    "            peak_pos.append(\"--\") \n",
    "        else:\n",
    "            peak_pos.append(i.text)\n",
    "    print(len(peak_pos),peak_pos)\n",
    "\n",
    "    #scraping the Weeks_on_board \n",
    "    wob=driver.find_elements(By.XPATH, '//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div/ul/li[4]/ul/li[6]/span')\n",
    "    for i in tqdm(wob):\n",
    "        if i.text is None :\n",
    "            weeks_on_board.append(\"--\") \n",
    "        else:\n",
    "            weeks_on_board.append(i.text)\n",
    "    print(len(weeks_on_board),weeks_on_board)\n",
    "    \n",
    "    df = pd.DataFrame({'Song Name':song_name, 'Singer':singer,'Last Week Rank': last_week, 'Peak Rank' : peak_pos, 'Weeks on Board': weeks_on_board})\n",
    "    display(df)\n",
    "scrape_billboard_top_songs(\"https://www.billboard.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683db62",
   "metadata": {},
   "source": [
    "### 6. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare .  \n",
    "\n",
    "**You have to find the following details: \n",
    "Scrape the details of Highest selling novels.**\n",
    " \n",
    "**A) Book name  \n",
    "B) Author name  \n",
    "C) Volumes sold  \n",
    "D) Publisher  \n",
    "E) Genre**      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1323a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 56.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 62.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 58.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 44.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 43.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "#Let's maximize the automated chrome window\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "book_name = []\n",
    "author_name = []\n",
    "volumes_sold =[]\n",
    "publisher = []\n",
    "genre = []\n",
    "\n",
    "#scraping the Book_name \n",
    "bname=driver.find_elements(By.XPATH, '//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[2]')\n",
    "for i in tqdm(bname):\n",
    "    if i.text is None:\n",
    "        book_name.append(\"--\")\n",
    "    else: \n",
    "        book_name.append(i.text)\n",
    "print(len(book_name), book_name)\n",
    "\n",
    "#scraping the author_name\n",
    "auth_name = driver.find_elements(By.XPATH, '//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[3]')\n",
    "for i in tqdm(auth_name):\n",
    "    if i.text is None:\n",
    "        author_name.append(\"--\")\n",
    "    else: \n",
    "        author_name.append(i.text)\n",
    "print(len(author_name),author_name)\n",
    "\n",
    "#scraping the volumes_sold\n",
    "vs = driver.find_elements(By.XPATH, '//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[4]')\n",
    "for i in tqdm(vs):\n",
    "    if i.text is None:\n",
    "        volumes_sold.append(\"--\")\n",
    "    else:\n",
    "        volumes_sold.append(i.text)\n",
    "print(len(volumes_sold), volumes_sold)\n",
    "\n",
    "#scraping the publisher\n",
    "pub = driver.find_elements(By.XPATH, '//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[5]')\n",
    "for i in tqdm(pub):\n",
    "    if i.text is None:\n",
    "         publisher.append(\"--\")\n",
    "    else:\n",
    "        publisher.append(i.text)\n",
    "print(len(publisher),publisher)\n",
    "\n",
    "#Scraping the genre\n",
    "gen = driver.find_elements(By.XPATH, '//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[6]')\n",
    "for i in tqdm(gen):\n",
    "    if i.text is None:\n",
    "        genre.append(\"__\")\n",
    "    else:\n",
    "        genre.append(i.text)\n",
    "print(len(genre), genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34d148",
   "metadata": {},
   "source": [
    "### 7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "**Url = https://www.imdb.com/list/ls095964455/**\n",
    "\n",
    "**You have to find the following details:**  \n",
    "**A) Name  \n",
    "B) Year span  \n",
    "C) Genre  \n",
    "D) Run time  \n",
    "E) Ratings  \n",
    "F) Votes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "369882cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Game of Thrones', 'Stranger Things', 'The Walking Dead', '13 Reasons Why', 'The 100', 'Orange Is the New Black', 'Riverdale', \"Grey's Anatomy\", 'The Flash', 'Arrow', 'Money Heist', 'The Big Bang Theory', 'Black Mirror', 'Sherlock', 'Vikings', 'Pretty Little Liars', 'The Vampire Diaries', 'American Horror Story', 'Breaking Bad', 'Lucifer', 'Supernatural', 'Prison Break', 'How to Get Away with Murder', 'Teen Wolf', 'The Simpsons', 'Once Upon a Time', 'Narcos', 'Daredevil', 'Friends', 'How I Met Your Mother', 'Suits', 'Mr. Robot', 'The Originals', 'Supergirl', 'Gossip Girl', 'Sense8', 'Gotham', 'Westworld', 'Jessica Jones', 'Modern Family', 'Rick and Morty', 'Shadowhunters: The Mortal Instruments', 'The End of the F***ing World', 'House of Cards', 'Dark', 'Elite', 'Sex Education', 'Shameless', 'New Girl', 'Agents of S.H.I.E.L.D.', 'You', 'Dexter', 'Fear the Walking Dead', 'Family Guy', 'The Blacklist', 'Lost', 'Peaky Blinders', 'House M.D.', 'Quantico', 'Orphan Black', 'Homeland', 'Blindspot', 'Legends of Tomorrow', \"The Handmaid's Tale\", 'Chilling Adventures of Sabrina', 'The Good Doctor', 'Jane the Virgin', 'Glee', 'South Park', 'Brooklyn Nine-Nine', 'Under the Dome', 'The Umbrella Academy', 'True Detective', 'The OA', 'Desperate Housewives', 'Better Call Saul', 'Bates Motel', 'The Punisher', 'Atypical', 'Dynasty', 'This Is Us', 'The Good Place', 'Iron Fist', 'The Rain', 'Mindhunter', 'Revenge', 'Luke Cage', 'Scandal', 'The Defenders', 'Big Little Lies', 'Insatiable', 'The Mentalist', 'The Crown', 'Chernobyl', 'iZombie', 'Reign', 'A Series of Unfortunate Events', 'Criminal Minds', 'Scream: The TV Series', 'The Haunting of Hill House']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['(2011–2019)', '(2016–2024)', '(2010–2022)', '(2017–2020)', '(2014–2020)', '(2013–2019)', '(2017–2023)', '(2005– )', '(2014–2023)', '(2012–2020)', '(2017–2021)', '(2007–2019)', '(2011– )', '(2010–2017)', '(2013–2020)', '(2010–2017)', '(2009–2017)', '(2011– )', '(2008–2013)', '(2016–2021)', '(2005–2020)', '(2005–2017)', '(2014–2020)', '(2011–2017)', '(1989– )', '(2011–2018)', '(I) (2015–2017)', '(2015–2018)', '(1994–2004)', '(2005–2014)', '(2011–2019)', '(2015–2019)', '(2013–2018)', '(2015–2021)', '(2007–2012)', '(2015–2018)', '(2014–2019)', '(2016–2022)', '(2015–2019)', '(2009–2020)', '(2013– )', '(2016–2019)', '(2017–2019)', '(2013–2018)', '(2017–2020)', '(2018– )', '(2019–2023)', '(2011–2021)', '(2011–2018)', '(2013–2020)', '(2018–2024)', '(2006–2013)', '(2015–2023)', '(1999– )', '(2013–2023)', '(2004–2010)', '(2013–2022)', '(2004–2012)', '(2015–2018)', '(2013–2017)', '(2011–2020)', '(2015–2020)', '(2016–2022)', '(2017– )', '(2018–2020)', '(2017– )', '(2014–2019)', '(2009–2015)', '(1997– )', '(2013–2021)', '(2013–2015)', '(2019–2023)', '(2014– )', '(2016–2019)', '(2004–2012)', '(2015–2022)', '(2013–2017)', '(2017–2019)', '(2017–2021)', '(2017–2022)', '(2016–2022)', '(2016–2020)', '(2017–2018)', '(2018–2020)', '(2017–2019)', '(2011–2015)', '(2016–2018)', '(2012–2018)', '(2017)', '(2017–2019)', '(2018–2019)', '(2008–2015)', '(2016–2023)', '(2019)', '(2015–2019)', '(2013–2017)', '(2017–2019)', '(2005– )', '(2015–2019)', '(2018)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Drama, Mystery, Thriller', 'Drama, Mystery, Sci-Fi', 'Comedy, Crime, Drama', 'Crime, Drama, Mystery', 'Drama, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Crime', 'Action, Crime, Drama', 'Comedy, Romance', 'Drama, Mystery, Sci-Fi', 'Crime, Drama, Mystery', 'Action, Adventure, Drama', 'Drama, Mystery, Romance', 'Drama, Fantasy, Horror', 'Drama, Horror, Sci-Fi', 'Crime, Drama, Thriller', 'Crime, Drama, Fantasy', 'Drama, Fantasy, Horror', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Animation, Comedy', 'Adventure, Fantasy, Romance', 'Biography, Crime, Drama', 'Action, Crime, Drama', 'Comedy, Romance', 'Comedy, Drama, Romance', 'Comedy, Drama', 'Crime, Drama, Thriller', 'Drama, Fantasy, Horror', 'Action, Adventure, Drama', 'Drama, Romance', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Comedy, Drama, Romance', 'Animation, Adventure, Comedy', 'Action, Drama, Fantasy', 'Adventure, Comedy, Crime', 'Drama', 'Crime, Drama, Mystery', 'Crime, Drama, Thriller', 'Comedy, Drama', 'Comedy, Drama', 'Comedy, Romance', 'Action, Adventure, Drama', 'Crime, Drama, Romance', 'Crime, Drama, Mystery', 'Drama, Horror, Sci-Fi', 'Animation, Comedy', 'Crime, Drama, Mystery', 'Adventure, Drama, Fantasy', 'Crime, Drama', 'Drama, Mystery', 'Crime, Drama, Mystery', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Action, Crime, Drama', 'Action, Adventure, Drama', 'Drama, Sci-Fi, Thriller', 'Drama, Fantasy, Horror', 'Drama', 'Comedy', 'Comedy, Drama, Music', 'Animation, Comedy', 'Comedy, Crime', 'Drama, Mystery, Sci-Fi', 'Action, Adventure, Comedy', 'Crime, Drama, Mystery', 'Drama, Fantasy, Mystery', 'Comedy, Drama, Mystery', 'Crime, Drama', 'Drama, Horror, Mystery', 'Action, Crime, Drama', 'Comedy, Drama', 'Drama', 'Comedy, Drama, Romance', 'Comedy, Drama, Fantasy', 'Action, Adventure, Crime', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Drama, Mystery, Thriller', 'Action, Crime, Drama', 'Drama, Thriller', 'Action, Adventure, Crime', 'Crime, Drama, Mystery', 'Comedy, Drama, Thriller', 'Crime, Drama, Mystery', 'Biography, Drama, History', 'Drama, History, Thriller', 'Comedy, Crime, Drama', 'Drama', 'Adventure, Comedy, Drama', 'Crime, Drama, Mystery', 'Comedy, Crime, Drama', 'Drama, Horror, Mystery']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['57 min', '51 min', '44 min', '60 min', '43 min', '59 min', '45 min', '41 min', '43 min', '42 min', '70 min', '22 min', '60 min', '88 min', '44 min', '44 min', '43 min', '60 min', '49 min', '42 min', '44 min', '44 min', '43 min', '41 min', '22 min', '60 min', '49 min', '54 min', '22 min', '22 min', '44 min', '49 min', '45 min', '43 min', '42 min', '60 min', '42 min', '62 min', '56 min', '22 min', '23 min', '42 min', '25 min', '51 min', '60 min', '60 min', '45 min', '46 min', '22 min', '45 min', '45 min', '53 min', '44 min', '22 min', '43 min', '44 min', '60 min', '44 min', '42 min', '44 min', '55 min', '42 min', '42 min', '60 min', '60 min', '41 min', '60 min', '44 min', '22 min', '22 min', '43 min', '60 min', '55 min', '60 min', '45 min', '46 min', '45 min', '53 min', '30 min', '42 min', '45 min', '22 min', '55 min', '45 min', '60 min', '44 min', '55 min', '43 min', '50 min', '60 min', '45 min', '43 min', '58 min', '330 min', '42 min', '42 min', '50 min', '42 min', '45 min', '572 min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['9.2', '8.7', '8.1', '7.5', '7.6', '8.1', '6.5', '7.6', '7.5', '7.5', '8.2', '8.2', '8.7', '9.1', '8.5', '7.4', '7.7', '8', '9.5', '8.1', '8.4', '8.3', '8.1', '7.7', '8.7', '7.7', '8.8', '8.6', '8.9', '8.3', '8.4', '8.5', '8.3', '6.2', '7.5', '8.2', '7.8', '8.5', '7.9', '8.5', '9.1', '6.5', '8', '8.7', '8.7', '7.3', '8.3', '8.6', '7.8', '7.5', '7.7', '8.7', '6.8', '8.2', '8', '8.3', '8.8', '8.7', '6.7', '8.3', '8.3', '7.3', '6.8', '8.4', '7.4', '8', '7.9', '6.8', '8.7', '8.4', '6.5', '7.9', '8.9', '7.8', '7.6', '9', '8.1', '8.5', '8.2', '7.3', '8.7', '8.2', '6.4', '6.3', '8.6', '7.8', '7.3', '7.7', '7.2', '8.5', '6.5', '8.2', '8.6', '9.4', '7.8', '7.5', '7.8', '8.1', '7', '8.6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['2,193,972', '1,267,622', '1,041,373', '306,079', '265,164', '312,769', '150,644', '326,875', '362,142', '440,306', '504,993', '837,872', '606,628', '962,823', '559,039', '173,784', '336,310', '330,223', '2,022,598', '341,267', '464,840', '559,532', '159,876', '158,004', '422,552', '231,366', '449,655', '458,972', '1,040,825', '708,302', '442,327', '403,984', '142,638', '127,515', '184,323', '159,212', '236,531', '519,567', '221,273', '458,846', '563,719', '67,643', '207,773', '518,998', '417,830', '86,062', '309,160', '262,093', '236,872', '222,287', '283,208', '744,848', '137,404', '354,042', '269,461', '575,150', '599,697', '487,206', '62,705', '114,312', '352,447', '77,041', '108,166', '249,267', '102,789', '106,433', '55,736', '152,717', '393,180', '340,026', '109,973', '262,043', '605,178', '110,073', '134,667', '597,683', '113,141', '252,432', '98,118', '24,062', '151,622', '176,838', '136,069', '39,903', '312,800', '122,821', '136,143', '77,503', '112,958', '213,874', '31,114', '193,371', '233,193', '815,219', '71,778', '52,465', '64,487', '210,179', '43,698', '263,814']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "#Let's maximize the automated chrome window\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "name=[]\n",
    "year_span=[]\n",
    "genres=[]\n",
    "run_time=[]\n",
    "ratings=[]\n",
    "votes=[]\n",
    "\n",
    "\n",
    "#scraping the Name \n",
    "mname=driver.find_elements(By.XPATH, \"//div[@class='lister-item-content']/h3/a\")\n",
    "for i in tqdm(mname):\n",
    "    if i.text is None :\n",
    "        name.append(\"--\") \n",
    "    else:\n",
    "        name.append(i.text)\n",
    "print(len(name),name)\n",
    "\n",
    "#scraping the Year_span \n",
    "ys=driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\")\n",
    "for i in tqdm(ys):\n",
    "    if i.text is None :\n",
    "        year_span.append(\"--\") \n",
    "    else:\n",
    "        year_span.append(i.text)\n",
    "print(len(year_span),year_span)\n",
    "\n",
    "#scraping the Genres \n",
    "gnr=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in tqdm(gnr):\n",
    "    if i.text is None :\n",
    "        genres.append(\"--\") \n",
    "    else:\n",
    "        genres.append(i.text)\n",
    "print(len(genres),genres)\n",
    "\n",
    "#scraping the Run_time \n",
    "rt=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in tqdm(rt):\n",
    "    if i.text is None :\n",
    "        run_time.append(\"--\") \n",
    "    else:\n",
    "        run_time.append(i.text)\n",
    "print(len(run_time),run_time)\n",
    "\n",
    "#scraping the Ratings \n",
    "rate=driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "for i in tqdm(rate):\n",
    "    if i.text is None :\n",
    "        ratings.append(\"--\") \n",
    "    else:\n",
    "        ratings.append(i.text)\n",
    "print(len(ratings),ratings)\n",
    "\n",
    "#scraping the Votes \n",
    "v=driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']/p[4]/span[2]\")\n",
    "for i in tqdm(v):\n",
    "    if i.text is None :\n",
    "        votes.append(\"--\") \n",
    "    else:\n",
    "        votes.append(i.text)\n",
    "print(len(votes),votes)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5cb71",
   "metadata": {},
   "source": [
    "### 8. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/  \n",
    "**You have to find the following details:  \n",
    "A) Dataset name  \n",
    "B) Data type  \n",
    "C) Task  \n",
    "D) Attribute type  \n",
    "E) No of instances  \n",
    "F) No of attribute G) Year \n",
    "Note: - from the home page you have to go to the Show All Dataset page through code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c000970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Iris', 'Heart Disease', 'Adult', 'Dry Bean Dataset', 'Diabetes', 'Wine', 'Breast Cancer Wisconsin (Diagnostic)', 'Car Evaluation', 'Rice (Cammeo and Osmancik)', 'Mushroom']\n",
      "10 ['Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', '', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate']\n",
      "10 ['Classification', 'Classification', 'Classification', 'Classification', '', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification']\n",
      "10 ['Real', 'Categorical, Integer, Real', 'Categorical, Integer', 'Integer, Real', 'Categorical, Integer', 'Integer, Real', 'Real', 'Categorical', 'Real', 'Categorical']\n",
      "10 ['150 Instances', '303 Instances', '48.84K Instances', '13.61K Instances', '', '178 Instances', '569 Instances', '1.73K Instances', '3.81K Instances', '8.12K Instances']\n",
      "10 ['4 Attributes', '13 Attributes', '14 Attributes', '16 Attributes', '20 Attributes', '13 Attributes', '30 Attributes', '6 Attributes', '8 Attributes', '22 Attributes']\n",
      "10 ['7/1/1988', '7/1/1988', '5/1/1996', '9/14/2020', 'N/A', '7/1/1991', '11/1/1995', '6/1/1997', '10/6/2019', '4/27/1987']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "#Let's maximize the automated chrome window\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "dataset=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]\")\n",
    "dataset.click()\n",
    "time.sleep(1)\n",
    "\n",
    "cookies = driver.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/div/div[2]/button')\n",
    "cookies.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "dataset_name = []\n",
    "data_type = []\n",
    "task = []\n",
    "attribute_type = []\n",
    "no_of_instances = []\n",
    "no_of_attribute = []\n",
    "year = []\n",
    "                                \n",
    "x = driver.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]/div[2]/span[1]')\n",
    "driver.execute_script(\"arguments[0].click()\",x)\n",
    "\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#scraping the dataset_name \n",
    "dname=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div/div[2]/h2\")\n",
    "for i in dname:\n",
    "    if i.text is None :\n",
    "        dataset_name.append(\"--\") \n",
    "    else:\n",
    "        dataset_name.append(i.text)\n",
    "print(len(dataset_name),dataset_name)\n",
    "\n",
    "#scraping the data_type \n",
    "dtype=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div/div[2]/div/div[2]/span\")\n",
    "for i in dtype:\n",
    "    if i.text is None :\n",
    "        data_type.append(\"--\") \n",
    "    else:\n",
    "        data_type.append(i.text)\n",
    "print(len(data_type),data_type)\n",
    "\n",
    "#scraping the task \n",
    "t=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div/div[2]/div/div[1]/span\")\n",
    "for i in t:\n",
    "    if i.text is None :\n",
    "        task.append(\"--\") \n",
    "    else:\n",
    "        task.append(i.text)\n",
    "print(len(task),task)\n",
    "\n",
    "#scraping the attribute_type \n",
    "att=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[2]\")\n",
    "for i in att:\n",
    "    if i.text is None :\n",
    "        attribute_type.append(\"--\") \n",
    "    else:\n",
    "        attribute_type.append(i.text)\n",
    "print(len(attribute_type),attribute_type)\n",
    "\n",
    "\n",
    "#scraping the no_of_instances \n",
    "noi=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div/div[2]/div/div[3]/span\")\n",
    "for i in noi:\n",
    "    if i.text is None :\n",
    "        no_of_instances.append(\"--\") \n",
    "    else:\n",
    "        no_of_instances.append(i.text)\n",
    "print(len(no_of_instances),no_of_instances)\n",
    "\n",
    "#scraping the no_of_attribute \n",
    "noa=driver.find_elements(By.XPATH, \"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div/div[2]/div/div[4]/span\")\n",
    "for i in noa:\n",
    "    if i.text is None :\n",
    "        no_of_attribute.append(\"--\") \n",
    "    else:\n",
    "        no_of_attribute.append(i.text)\n",
    "print(len(no_of_attribute),no_of_attribute)\n",
    "\n",
    "#scraping the year \n",
    "y=driver.find_elements(By.XPATH, \"//html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[3]\")\n",
    "for i in y:\n",
    "    if i.text is None :\n",
    "        year.append(\"--\") \n",
    "    else:\n",
    "        year.append(i.text)\n",
    "print(len(year),year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8099a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
